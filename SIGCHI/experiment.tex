\section{Experiment}

We performed a between-subjects online experiments with young adults. Participants were randomly assigned to one of two conditions: MindMargin or a vertical commenting interface.

\minisubsection{Participants}
106 online participants landed on our page for our user study and evaluation, of which 46 proceeded to begin and complete the study (30 female). 19 participants were assigned to the Mind Margin condition and 27 to the vertical interface condition.  Participants were recruited online through social media and college listservs. Participants were college students, aged 18 to 25, and 68\% hailed from the local university. The self-reported reading frequency of online news among participants ranged from daily to almost never. 

\minisubsection{Experimental Conditions}
The two conditions in our study were MindMargin and the traditional vertical interface, both seeded with existing comments from a relevant news article. The article was selected on the basis of its opinionated nature and its relevance both in recent news and to our anticipated participant pool. We chose an opinion piece from our university's undergraduate publication, titled “Don't Teach for America.” 

The article already had over fifty comments by affiliates and non-affiliates of the university alike, from which we selected the top 39 comments as ranked by Disqus, the existing commenting system on the publication's website, to be used in our study. The same comments were used in both conditions. In the traditional vertical interface, the comments appeared in the identical order as ranked in the original article. In MindMargin, we anchored them to the article based on textual references, specific phrases, quotes, and relevant content in each comment. 

Users could make new comments by writing in the static new comment box above existing comments in the traditional interface and by clicking any part of the article to open a new comment box in MindMargin. In the MindMargin condition, we provided participants with simple, temporary instructions: "click the text to comment."

XXXX include that last paragraph? XXXX

\minisubsection{Tasks}
To ensure that our results would be informative for the design of real-world commenting systems, we designed the experimental tasks to focus participants' attention on the content of the article.  The study design did not emphasize that the evaluation of the commenting system was the object of the study ([[XXX but was it disclosed in the consent form?]]

Participants were presented with an article and they were instructed to [[XXX ??? XXX]].  Once they completed reading the article, we asked them verification questions to ensure that they have indeed read and comprehended the article.  Specifically we asked them about the overall stance of the article and, in a free-text response, for two pieces of supporting evidence used in the article.  All 46 participants gave correct and thorough answers to these verification questions.  Participants were then asked to complete a post-experiment questionnaire and were not permitted to refer back to the article once the questionnaire was administered. [[XXX state what questions were included in this questionnaire]]

To further incentivize focus on the content of the article and reflection on the issue it discussed, we used the following tagline to advertise the study: "Do you (really) think like a Harvard student?" and at the end of the study we provided them with feedback showing how their stance on the issue discussed in the article compared to those of other participants [[XXX other participants or Harvard students?]].

%In order to reproduce the conditions under which one would normally read a news article, we chose to recruit participants online and to allow them to self-select themselves into reading the article based on personal time and interest. In order to motivate our participants to actually read or skim the article, instead of skip it, we chose not to use monetary or other time-sensitive incentives. Instead, we chose to design the experiment around the survey question, "Do you (really) think like a student [from our local university]?" 
%
%We then asked participants follow-up questions to verify that they read the article, which included both the overall stance of the article and, in a free-text response, two pieces of supporting evidence used in the article. All 46 participants gave correct and thorough answers to these verification questions. Participants were then asked to complete a post-experiment questionnaire and were not permitted to refer back to the article once the questionnaire was administered.

\minisubsection{Procedure}
Participants were given an initial questionnaire asking basic demographics and reading frequency. Before given the article, they were also asked either to provide a username or pseudonym, or to remain anonymous. During the reading of the article, participants were allotted 10 minutes. After 2 minutes, they were permitted to proceed to the questionnaire. The 2-minute delay was to ensure the reading of the article, and did not seem to prevent fast readers from moving too slowly, as the average reading time was 3 minutes 47 seconds. 

In the follow-up questionnaire, reading verification questions were first posed. Participants were then asked their personal stance on the article, whether they liked the article, and whether they agreed with the article. They were also asked to self-report whether they read the comments in the article and to provide two adjectives that described either their reaction to, or a description of, the comments.

\minisubsection{Design and Analysis}
We use a between-subject factorial design w/ 2 factors
1st: condition
2nd factor: prior exposure to article
and we used the following metrics/measures: ... extreme factor + and a number of subjective measures (adjectives==state that we coded these adjectives; discuss agreement across coders)
explain what kinds of statistical tests we use
